{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Notes\n",
    "\n",
    "- To run this notebook you will need to [install jupyter](https://jupyter.org/install)\n",
    "    - If working in VSCode (which we recommend), you will need to activate your python environment by selecting the appropriate kernel\n",
    "- The functions you need to implement are **very short**. If the first thing you do is start writing code you will struggle. You need to understand the **inputs and outputs** of each method and understand how to use **numpy** operations to avoid the use of for loops.\n",
    "- You **do not** need to submit to gradescope, we supply all necessary tests in tests.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting up linear regression\n",
    "\n",
    "In this part of the assignment you will set up the necessary code for doing linear regression. Namely you will implement:\n",
    "- create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "- get_simple_linear_features(x)\n",
    "- linear_prediction(x, A, get_modified_features)\n",
    "- mse_loss(y_pred, y_true)\n",
    "- compute_model_error(x, y, A, get_modified_features)\n",
    "\n",
    "We'll tackle these one at a time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some generic useful imports - this is ALL you need\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tests\n",
    "# the lines below ensure that you can modify mp9.py and imported files update accordingly\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_linear_data\n",
    "\n",
    "To do linear regression (or any machine learning) we need data. \n",
    "\n",
    "For now you will be *synthesizing* your own data. Given a `slope` and `intercept` you will create and return artificial `x` and noisy `y` values. `x` should be uniformly sampled between `x_range[0]` and `x_range[1]`. You should create `num_samples` `x` values, compute `y = slope * x + intercept`, and then add some uniform noise sampled between `-noise` and `noise`.\n",
    "\n",
    "You will find `np.random.rand` useful for sampling both the `x` values and the uniform noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the create_linear_data function\n",
    "# NOTE: DO NOT use for loops\n",
    "from mp9 import create_linear_data\n",
    "\n",
    "num_samples = 100\n",
    "slope = 2\n",
    "intercept = 3\n",
    "x_range = [0, 10]\n",
    "noise = 0.5\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "# plot\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_create_linear_data(num_samples=10, slope=2, intercept=-1, x_range=[-10,10], noise=0)\n",
    "tests.test_create_linear_data(num_samples=10, slope=-3, intercept=2, x_range=[0,5], noise=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_simple_linear_features and linear_prediction\n",
    "\n",
    "An extremely important concept in machine learning is **vectorization**.\n",
    "\n",
    "Instead of writing $y_i=mx_i+b$ for some data $(x_i, y_i)$ we prefer writing $A=\\begin{bmatrix} m \\\\ b \\end{bmatrix}, X_i=\\begin{bmatrix} x_i & 1 \\end{bmatrix}$, and thus $y_i=X_iA$. Note that $X_i$ is a $(1,2)$ matrix and $A$ is a $(2,1)$ matrix and so the output $y_i$ is $(1,1)$.\n",
    "\n",
    "Having written things in this way we now no longer think of \"slopes\" and \"intercepts\" but rather generic \"features\". Our features ($X_i$) get multipled by these **parameters** ($A$) to compute our output ($y_i$). After this transformation we can think of our input as $2$-dimensional (even though the second dimension is always equal to $1$) and our output as $1$-dimensional. Later in this assignment you'll see how we can modify these features further.\n",
    "\n",
    "Now we can go further and write down predicted y values for the entire dataset as one matrix multiplication!\n",
    "$$Y = \\begin{bmatrix} y_{0} \\\\ y_{1} \\\\ \\vdots \\\\ y_{n-1} \\end{bmatrix}, X = \\begin{bmatrix} x_{0} & 1 \\\\ x_{1} & 1 \\\\ \\vdots & \\vdots \\\\ x_{n-1} & 1 \\end{bmatrix} \\implies Y = XA$$\n",
    "\n",
    "We can now basically summarize our goal as \"find the matrix A which gives good predictions across our dataset\". We'll look at what \"good\" means in the next section but first let's implement some methods that compute $X$ given $x$ and predict $Y$ given $X$ and $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the get_simple_linear_features and linear_prediction functions\n",
    "# NOTE: our solution is ONE line of code for each function! DO NOT use for loops.\n",
    "#       You should use methods like np.concatenate, np.ones, and np.dot\n",
    "from mp9 import get_simple_linear_features, linear_prediction\n",
    "\n",
    "# get x_values for plotting the line\n",
    "x_plot = np.linspace(x_range[0], x_range[1], 1000).reshape(-1, 1)\n",
    "# now make a matrix A of shape (2, 1) with the slope and intercept\n",
    "A_true = np.array([[slope], [intercept]])\n",
    "# now make a prediction\n",
    "y_hat = linear_prediction(x_plot, A_true, get_simple_linear_features)\n",
    "# now plot the data and the prediction\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"true_line\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_get_simple_linear_features(num_samples=10, num_features=1)\n",
    "tests.test_get_simple_linear_features(num_samples=1, num_features=4)\n",
    "tests.test_linear_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mse_loss and compute_model_error \n",
    "\n",
    "Above we said our goal is to find parameters that make \"good\" predictions. But what does good mean? In our case, we will look for the set of parameters that *minimize a loss function*. In particular we will use the Mean Squared Error:\n",
    "$$MSE(Y, \\bar Y) = \\frac{1}{n}\\sum_i (y_i - \\bar{y_i})^2$$\n",
    "where $\\bar Y$ are the true values and $Y$ are our predictions.\n",
    "\n",
    "Thanks to our vectorization from before we can simply rewrite this as:\n",
    "$$MSE(Y, \\bar Y) = \\frac{1}{n}||Y - \\bar Y||_2^2 = \\frac{1}{n}(Y - \\bar Y)^T(Y - \\bar Y)$$\n",
    "\n",
    "and since our predictions are $Y = XA$ our model error is:\n",
    "$$L(A, X, \\bar Y) = \\frac{1}{n}||XA - \\bar Y||_2^2$$\n",
    "\n",
    "NOTE: The notation $||\\cdot||_2$ is the $2$-norm, or Euclidean norm, of a vector. So the loss above is the squared $2$-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the mse_loss function\n",
    "# NOTE: our solution is ONE line of code! Use numpy operations not for loops!\n",
    "from mp9 import compute_model_error\n",
    "\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "\n",
    "perfect_model = np.array([[slope], [intercept]])\n",
    "perfect_error = compute_model_error(x, y, perfect_model, get_simple_linear_features)\n",
    "\n",
    "bad_model = np.array([[slope + 1], [intercept + 1]])\n",
    "bad_error = compute_model_error(x, y, bad_model, get_simple_linear_features)\n",
    "\n",
    "really_bad_model = np.array([[-slope], [-intercept]])\n",
    "really_bad_error = compute_model_error(x, y, really_bad_model, get_simple_linear_features)\n",
    "\n",
    "# NOTE: perfect error is not zero because of the noise\n",
    "print(f\"Perfect model error: {perfect_error}\")\n",
    "print(f\"Bad model error: {bad_error}\")\n",
    "print(f\"Really bad model error: {really_bad_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_mse_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analytical linear regression\n",
    "\n",
    "Great, so we have data, we can vectorize it, and we can compute some error. But how do we actually find the right set of parameters? The answer is to *minimize the loss*. There are many ways to minimize a function, first we'll look at a simple approach from calculus: to minimize a function take the derivative, set it equal to zero, and solve the equation.\n",
    "\n",
    "Specifically, we take the partial derivative of our *loss* with respect to our *parameters*:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{\\partial}{\\partial A} \\left(\\frac{1}{n}||XA-Y||_2^2\\right)$$\n",
    "\n",
    "$$= \\frac{2}{n}X^T(XA-Y) = 0$$\n",
    "$$\\implies X^T(XA-Y) = 0$$\n",
    "$$\\implies X^TXA-X^TY = 0$$\n",
    "$$\\implies X^TXA = X^TY$$\n",
    "$$\\implies A = (X^TX)^{-1}X^T Y$$\n",
    "\n",
    "You're not expected to know/understand matrix derivatives but you can intuitively see how we got the above derivative by imagining these were real numbers: if we had no matrices, our loss would maybe look like $L(a,x,y) = (ax - y)^2$, and the derivative of this with respect to $a$ is just $2x(ax-y)$ (which is basically what we got above just without any transposes...).\n",
    "\n",
    "Fill in the function `analytical_linear_regression` with the above expression to compute $A$ given $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the analytical_linear_regression function. DO NOT use for loops!\n",
    "from mp9 import analytical_linear_regression\n",
    "# create data\n",
    "num_samples = 100\n",
    "slope = 2\n",
    "intercept = 3\n",
    "x_range = [0, 10]\n",
    "noise = 0.5\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "# transform x into features (add column of 1's)\n",
    "X = get_simple_linear_features(x)\n",
    "# find the analytical solution\n",
    "predicted_A = analytical_linear_regression(X, y)\n",
    "print(f\"True slope: {slope}, True intercept: {intercept}\")\n",
    "print(f\"Predicted slope: {predicted_A[0, 0]}, Predicted intercept: {predicted_A[1, 0]}\")\n",
    "\n",
    "# plot\n",
    "y_hat = linear_prediction(x_plot, predicted_A, get_simple_linear_features)\n",
    "y_true = linear_prediction(x_plot, perfect_model, get_simple_linear_features)\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"predicted_line\")\n",
    "plt.plot(x_plot, y_true, color='green', label=\"true_line\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Now see what happens with larger noise and few samples\n",
    "num_samples = 20\n",
    "large_noise = 5.0\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, large_noise)\n",
    "X = get_simple_linear_features(x)\n",
    "predicted_A = analytical_linear_regression(X, y)\n",
    "print(f\"True slope: {slope}, True intercept: {intercept}\")\n",
    "print(f\"Predicted slope: {predicted_A[0, 0]}, Predicted intercept: {predicted_A[1, 0]}\")\n",
    "\n",
    "# plot\n",
    "y_hat = linear_prediction(x_plot, predicted_A, get_simple_linear_features)\n",
    "y_true = linear_prediction(x_plot, perfect_model, get_simple_linear_features)\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"predicted_line\")\n",
    "plt.plot(x_plot, y_true, color='green', label=\"true_line\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_analytical_linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient descent\n",
    "\n",
    "In practice (i.e., with neural networks) we can *take* the derivative but *NOT solve for when it is equal to zero*. Now you will implement an algorithm, gradient descent, which uses the derivative to iteratively update the parameters. As above, our derivative is:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial A} = \\frac{2}{n}X^T(XA-Y)$$\n",
    "\n",
    "Gradient descent simply computes:\n",
    "$$A_{new} = A_{old} - \\alpha \\frac{\\partial L}{\\partial A_{old}}$$\n",
    "\n",
    "where $\\alpha$ is a learning rate. \n",
    "\n",
    "Now implement `get_linear_regression_gradient` and `gradient_descent` functions...\n",
    "- Notice how gradient_descent takes as input a function called get_gradient which only depends on the parameters - see the code example below for how we pass `get_linear_regression_gradient` into `gradient_descent`\n",
    "- Notice the TODO below - you should modify the `learning_rate` and `num_iterations` yourself until you get good results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp9 import get_linear_regression_gradient, gradient_descent\n",
    "\n",
    "# initialize data\n",
    "num_samples = 100\n",
    "slope = 2\n",
    "intercept = 3\n",
    "x_range = [0, 10]\n",
    "noise = 0.5\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "X = get_simple_linear_features(x)\n",
    "\n",
    "# random initialization of parameters\n",
    "A_init = np.random.randn(2, 1)\n",
    "\n",
    "# TODO: play around with these parameters to see how they affect the results\n",
    "learning_rate = 1.0\n",
    "num_iterations = 1\n",
    "\n",
    "# *** notice how we fix X and y as inputs to get_linear_regression_gradient in order to compute get_gradient ***\n",
    "get_gradient = lambda A: get_linear_regression_gradient(A, X, y)\n",
    "\n",
    "# do gradient descent\n",
    "predicted_A = gradient_descent(get_gradient, A_init, learning_rate, num_iterations)\n",
    "\n",
    "# plot\n",
    "y_hat = linear_prediction(x_plot, predicted_A, get_simple_linear_features)\n",
    "y_true = linear_prediction(x_plot, perfect_model, get_simple_linear_features)\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"predicted_line\")\n",
    "plt.plot(x_plot, y_true, color='green', label=\"true_line\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_linear_regression_gradient()\n",
    "tests.test_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent (SGD)\n",
    "\n",
    "Imagine we had a $1,000,000$ data points (which is a small fraction of the amount of data used to train models like GPT). In every iteration of gradient descent we did computation using all the data at once only to make a small update to our parameters. This can be quite slow - for example if we had $100$ features (GPT uses thousands) this would entail doing over $100,000,000$ mathematical operations just to compute $XA$!\n",
    "\n",
    "Stochastic gradient descent makes a simple change to gradient descent - each time we want to update our parameters we will sample a batch of data from our dataset and compute the gradient using just that batch. This means our gradient is a *worse approximation of the true gradient* (remember that our data has noise and so our gradient is noisy). As a result we usually have to lower the learning rate a little (you can think of learning rate as how much you trust the direction the gradient is telling you to go).\n",
    "\n",
    "Now implement SGD - each epoch should represent one pass through the entire dataset, and each pass should randomly order the data and then update in batches.\n",
    "- Notice how `stochastic_gradient_descent` uses a `get_batch_gradient` function instead of `get_gradient`. The only difference is that `get_batch_gradient` takes as input a list of indices and then subselects from the data to compute the gradient. Again, see below for sample usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the stochastic_gradient_descent function\n",
    "from mp9 import stochastic_gradient_descent\n",
    "\n",
    "# initialize data\n",
    "num_samples = 100\n",
    "slope = 2\n",
    "intercept = 3\n",
    "x_range = [0, 10]\n",
    "noise = 0.5\n",
    "x, y = create_linear_data(num_samples, slope, intercept, x_range, noise)\n",
    "X = get_simple_linear_features(x)\n",
    "\n",
    "# stochastic gradient descent parameters \n",
    "A_init = np.random.randn(2, 1)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "data_size = len(x)\n",
    "# *** notice how we now take indices as input to get a subset of the data ***\n",
    "get_batch_gradient = lambda A, indices: get_linear_regression_gradient(A, X[indices], y[indices])\n",
    "\n",
    "predicted_A = stochastic_gradient_descent(\n",
    "    get_batch_gradient, A_init, learning_rate, num_epochs, data_size, batch_size)\n",
    "\n",
    "# plot\n",
    "y_hat = linear_prediction(x_plot, predicted_A, get_simple_linear_features)\n",
    "y_true = linear_prediction(x_plot, perfect_model, get_simple_linear_features)\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"predicted_line\")\n",
    "plt.plot(x_plot, y_true, color='green', label=\"true_line\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_stochastic_gradient_descent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sine data and feature transforms\n",
    "\n",
    "Now we will fit a function to sine data still using linear regression. How is this possible - a line cannot fit a sine curve?! \n",
    "\n",
    "First let's create synthetic data. The `create_sine_data(num_samples, x_range, noise)` function should create data of the form $y_i = \\sin(x_i) + \\epsilon_i$ for:\n",
    "- $0 \\leq i <$ num_samples\n",
    "- $x_i \\sim U(x\\_range[0], x\\_range[1])$\n",
    "- $\\epsilon_i \\sim U(-noise, noise)$.\n",
    "\n",
    "where $U(a,b)$ means uniform distribution between $a$ and $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the create_sine_data function\n",
    "from mp9 import create_sine_data\n",
    "\n",
    "num_samples = 100\n",
    "x_range = [0, 2 * np.pi]\n",
    "noise = 0.1\n",
    "x, y = create_sine_data(num_samples, x_range, noise)\n",
    "\n",
    "# plot\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_create_sine_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_polynomial_features\n",
    "\n",
    "Obviously a linear function cannot fit this data... but a polynomial function could. In order to do polynomial regression we would need to modify our model from $y=mx+b$ to $y=a_k x^k + a_{k-1} x^{k-1} + \\dots + a_1 x + a_0$ for some degree $k$.\n",
    "\n",
    "The easiest way to think of this is as a feature transform. Before we \"transformed\" our features as follows:\n",
    "\n",
    "$$X = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n-1} \\end{bmatrix} \\to X_{linear} = \\begin{bmatrix} x_{0} & 1 \\\\ x_{1} & 1 \\\\ \\vdots & \\vdots \\\\ x_{n-1} & 1 \\end{bmatrix}$$\n",
    "\n",
    "We can actually think of this as:\n",
    "$$X_{linear} = \\begin{bmatrix} X^1 & X^0 \\end{bmatrix}$$\n",
    "\n",
    "Now we will compute a new more general transform:\n",
    "$$X_{polynomial} = \\begin{bmatrix} X^k & X^{k-1} & \\dots & X^0 \\end{bmatrix}$$\n",
    "\n",
    "For example, if $X$ has $1$-dimensional features...\n",
    "$$X = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n-1} \\end{bmatrix}\\implies X_{polynomial} = \\begin{bmatrix} x_0^k & x_0^{k-1} & \\dots & x_{0} & 1 \\\\ x_1^k & x_1^{k-1} & \\dots & x_{1} & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ x_{n-1}^k & x_{n-1}^{k-1} & \\dots & x_{n-1} & 1 \\end{bmatrix}$$\n",
    "\n",
    "Or if $X$ has $2$-dimensional features...\n",
    "$$X = \\begin{bmatrix} x_{0,0} & x_{0,1} \\\\ x_{1,0} & x_{1,1} \\\\ \\vdots \\\\ x_{n-1,0} & x_{n-1,1} \\end{bmatrix}\\implies X_{polynomial} = \\begin{bmatrix} x_{0,0}^k & x_{0,1}^k & x_{0,0}^{k-1} & x_{0,1}^{k-1} \\dots & x_{0,0} & x_{0,1} & 1 & 1 \\\\ x_{1,0}^k & x_{1,1}^k & x_{1,0}^{k-1} & x_{1,1}^{k-1} \\dots & x_{1,0} & x_{1,1} & 1 & 1 \\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\ x_{n-1,0}^k & x_{n-1,1}^k & x_{n-1,0}^{k-1} & x_{n-1,1}^{k-1} \\dots & x_{n-1,0} & x_{n-1,1} & 1 & 1 \\end{bmatrix}$$\n",
    "Though it is a lot simpler to think of this as stacked exponents of the entire matrix $X$.\n",
    "\n",
    "- Notice that if $X$ has shape $(n, m)$ then $X_{polynomial}$ has shape $(n, m(k+1))$, which means our parameter matrix $A$ will now have shape $(m(k+1), 1)$. After this feature transform we can once again do plain old linear regression, meaning compute $A$ such that the MSE loss is minimized for model $Y = XA$...\n",
    "- Notice the TODO below, you should play around with the degree of the polynomial (obviously `degree=1` is insufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the get_polynomial_features function\n",
    "from mp9 import get_polynomial_features\n",
    "\n",
    "# TODO: you should change the degree to see how the model fits the data\n",
    "degree = 1\n",
    "X = get_polynomial_features(x, degree=degree)\n",
    "# NOTE: we're using analytical linear regression here, \n",
    "# but you can substitute in gradient descent or stochastic gradient descent\n",
    "A_sine = analytical_linear_regression(X, y)\n",
    "\n",
    "# plot\n",
    "x_plot = np.linspace(x_range[0], x_range[1], 1000).reshape(-1, 1)\n",
    "y_hat = linear_prediction(x_plot, A_sine, lambda x: get_polynomial_features(x, degree=degree))\n",
    "plt.plot(x_plot, y_hat, color='red', label=\"prediction\")\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_get_polynomial_features(num_data=10, num_features=1, degree=3)\n",
    "tests.test_get_polynomial_features(num_data=10, num_features=2, degree=3)\n",
    "tests.test_get_polynomial_features(num_data=100, num_features=1, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inverse kinematics via gradient descent\n",
    "\n",
    "The gradient descent algorithm is extremely general and is used in a wide range of applications. We will now see a (simplified) application to robotics, namely inverse kinematics. \n",
    "\n",
    "In robotics, forward kinematics refers to the process of computing geometries from configurations. For example, if our robot is a rectangle at position $(x,y,\\theta)$, where $\\theta$ is its orientation, then forward kinematics here refers to finding the coordinates of the four corners of this rotated rectangle. Or if our robot is an arm with three joints described by their respective angles $(\\theta_1, \\theta_2, \\theta_3)$, then forward kinematics is computing the three line segments (or cylinders, depending on how we represent our robot) for the three joints.\n",
    "\n",
    "Inverse kinematics is the reverse - given a desired position in the real world compute a configuration that achieves this position. For example, in the case of the 3 joint arm, given an $(x,y)$ coordinate for the tip of the arm, also called the end-effector, find joint angles such that the end-effector is at that coordinate. This is a one-to-many function. Think of putting your hand on a table and then moving your elbow - your hand stays put but the joint configuration of your arm changes.\n",
    "\n",
    "Let's meet our robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: you don't need to write any code, but you should familiarize yourself with the Arm class\n",
    "#       namely you should understand what the code below is doing, you may want to play around with it a little\n",
    "from planar_arm import Arm\n",
    "\n",
    "# create an Arm object with 3 joints, each has length 1\n",
    "num_joints = 3\n",
    "arm_lengths = np.ones(num_joints)\n",
    "arm = Arm(arm_lengths=arm_lengths)\n",
    "# draw_space sets the axis limits\n",
    "arm.draw_space(plt.gca())\n",
    "\n",
    "# now define a configuration for the arm - meaning 3 joint angles\n",
    "# NOTE: joint angles are in degrees\n",
    "config = np.random.rand(num_joints) * 360\n",
    "# draw the configuration\n",
    "arm.draw_config(plt.gca(), config, color='blue', label='arm_config')\n",
    "plt.scatter(0, 0, color='black', label='arm_base')\n",
    "\n",
    "# compute the end effector position\n",
    "# forward_kinematics returns the (x,y) positions of each joint and the end effector\n",
    "forward_kinematics = arm.forward_kinematics(config)\n",
    "print(f\"Our robot has {forward_kinematics.shape} endpoints\")\n",
    "end_effector = forward_kinematics[-1]\n",
    "print(\"The arm base is fixed at (0,0)\")\n",
    "print(f\"The end effector position is {end_effector}\")\n",
    "# plot the end effector\n",
    "plt.scatter(end_effector[0], end_effector[1], color='red', label='end_effector')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ik_loss\n",
    "\n",
    "In order to do gradient descent we need a loss function. Fill in the `ik_loss` function which should simply compute the euclidean distance between the end effector and some goal position (you may find `np.linalg.norm` useful).\n",
    "\n",
    "Also note we've provided a more complex loss function that uses ik_loss and also computes a loss with respect to obstacles. The code below samples random configurations of the arm until one of them has a loss less than $0.1$ (meaning the end effector is close to the goal position). This is one way of doing inverse kinematics but we'll discuss below a better alternative.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the ik_loss function\n",
    "from mp9 import ik_loss\n",
    "\n",
    "# let's say we want the end effector to be at (1, 1)\n",
    "target = np.array([1, 1])\n",
    "\n",
    "# we will keep sampling configurations until we get close...\n",
    "config = np.random.rand(num_joints) * 360\n",
    "num_attempts = 0\n",
    "max_error = 0.1\n",
    "while ik_loss(arm, config, target) > max_error:\n",
    "    config = np.random.rand(num_joints) * 360\n",
    "    num_attempts += 1\n",
    "final_loss = ik_loss(arm, config, target)\n",
    "print(f\"Final loss after {num_attempts} samples: {final_loss}\")\n",
    "\n",
    "arm.draw_space(plt.gca())\n",
    "arm.draw_config(plt.gca(), config, color='blue', label='arm_config')\n",
    "plt.scatter(0, 0, color='red', label='arm_base')\n",
    "plt.scatter(target[0], target[1], color='black', label='target')\n",
    "end_effector = arm.forward_kinematics(config)[-1]\n",
    "plt.scatter(end_effector[0], end_effector[1], color='green', label='end_effector')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_ik_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate_ik_gradient and sample_near\n",
    "\n",
    "Sampling randomly works but is not a great strategy for two main reasons:\n",
    "- As the number of joints increases we will need exponentially more samples - try it above. \n",
    "- In practice we not only want the final configuration but a path to it. In previous assignments you saw how we could do this by discretizing the continuous space. Now we will see how gradient descent can solve this same planning problem. \n",
    "\n",
    "Gradients can be computed (as we did earlier) but they can also be approximated. In simplest terms, our goal is to know which direction to update our parameters such that our function value decreases. This is exactly what the negative gradient describes (which is why gradient descent does \"minus\"). We will now approximate the direction to update by sampling nearby parameter values and finding the one that gives the largest decrease in loss.\n",
    "\n",
    "Implement `sample_near`, a function which takes a configuration (in our case the configuration represents our parameter values) and samples nearby configurations uniformly.\n",
    "\n",
    "Next implement `estimate_ik_gradient`, a function which samples near a configuration, computes the loss at these neighboring configurations, and returns a (**unit**) vector pointing from the current configuration to the sample with **highest** loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the estimate_ik_gradient function\n",
    "from mp9 import estimate_ik_gradient\n",
    "\n",
    "# let's say we want the end effector to be at (1, 1)\n",
    "target = np.array([1, 1])\n",
    "# we sample num_sample_near configurations to estimate the gradient at each point\n",
    "num_sample_near = 20\n",
    "# our gradient with respect to a config given the target and loss function\n",
    "loss = lambda config: ik_loss(arm, config, target)\n",
    "get_gradient = lambda config: estimate_ik_gradient(loss, config, num_samples=num_sample_near)\n",
    "\n",
    "# gradient descent parameters\n",
    "config_init = np.random.rand(num_joints) * 360 # random initialization, this is like A_init\n",
    "learning_rate = 1.0 # notice a much higher learning rate\n",
    "num_iterations = 1000 # with 1000 iterations we will get much closer to the target than before with over 1000 samples\n",
    "final_config = gradient_descent(get_gradient, config_init, learning_rate, num_iterations)\n",
    "\n",
    "# plot\n",
    "arm.draw_space(plt.gca())\n",
    "arm.draw_config(plt.gca(), final_config, color='green', label='final_config')\n",
    "arm.draw_config(plt.gca(), config_init, color='blue', label='initial_config')\n",
    "plt.scatter(target[0], target[1], color='black', label='target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_sample_near()\n",
    "tests.test_estimate_ik_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This just shows the initial and final configurations... If you want to see the entire path you can run gradient descent for a few steps and plot each time. If your code above works then you can run `main.py` where we do this for you.\n",
    "\n",
    "`python3 main.py --data_type ik --method gd --learning_rate 1 --batch_size 20 --plot_iterations 5 --num_iterations 1000`\n",
    "\n",
    "---\n",
    "\n",
    "Let's now see what happens when we add obstacles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp9 import ik_loss_with_obstacles\n",
    "from main import run_optimization_and_save_intermediates\n",
    "\n",
    "num_joints = 6 # what if you made this 6?\n",
    "arm_lengths = np.ones(num_joints)\n",
    "arm = Arm(arm_lengths=arm_lengths)\n",
    "\n",
    "# let's say we want the end effector to be at (1, 1)\n",
    "target = np.array([1, 1])\n",
    "# obstacles are circles with (x, y, radius)\n",
    "obstacles = np.array([[-1.5, 1.5, 0.5]])\n",
    "# draw the obstacles\n",
    "for obstacle in obstacles:\n",
    "    plt.gca().add_patch(plt.Circle(obstacle[:2], obstacle[2], color='red', alpha=0.2))\n",
    "\n",
    "# we sample num_sample_near configurations to estimate the gradient at each point\n",
    "num_sample_near = 30\n",
    "# our gradient with respect to a config\n",
    "# loss = lambda config: ik_loss(arm, config, target)\n",
    "loss = lambda config: ik_loss_with_obstacles(arm, config, target, obstacles)\n",
    "get_gradient = lambda config: estimate_ik_gradient(loss, config, num_samples=num_sample_near)\n",
    "\n",
    "# we specifically start the arm to the left of the obstacle\n",
    "config_init = np.array([180, 30, 30])\n",
    "if num_joints == 6:\n",
    "    config_init = np.array([180, 30, 30, 0,0,0])\n",
    "\n",
    "learning_rate = 1.0\n",
    "total_iters = 2000\n",
    "\n",
    "# this is some code from main.py that runs the optimization and saves intermediate steps\n",
    "optim = lambda A_init, num_iterations: gradient_descent(\n",
    "            get_gradient=get_gradient,\n",
    "            A_init=A_init, \n",
    "            learning_rate=learning_rate, \n",
    "            num_iterations=num_iterations)\n",
    "A_vals = run_optimization_and_save_intermediates(\n",
    "    optim, config_init, total_iters=total_iters, plot_iters=20)\n",
    "\n",
    "for i, A_val in enumerate(A_vals):\n",
    "    plot_params = {\"color\": 'red', \"alpha\": 0.2}\n",
    "    if i == 0:\n",
    "        plot_params[\"label\"] = \"Intermediate\"\n",
    "    if i == len(A_vals) - 1:\n",
    "        plot_params = {\"color\": 'green', \"label\": \"Final\"}\n",
    "    arm.draw_config(plt.gca(), A_val, **plot_params)\n",
    "# plot\n",
    "arm.draw_space(plt.gca())\n",
    "arm.draw_config(plt.gca(), config_init, color='blue', label='initial_config')\n",
    "plt.scatter(target[0], target[1], color='black', label='target')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the loss function and obstacles... If you give the arm 6 joints does it still get to its goal (change the num_joints to 6)? Can you make it still get to the goal (e.g., try multiplying the obstacle loss by 100 in the `ik_loss_with_obstacles` function...)?\n",
    "\n",
    "Note: this is not necessary for passing our tests and is purely for your own enjoyment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Classification - logistic regression\n",
    "\n",
    "Above we looked at regression and inverse kinematics. Now we will briefly look at classification, where instead of trying to predict some real number for each input we need to predict a class (either 0 or 1). Also, this time, instead of creating artificial data we will load some data that has been provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import load_data\n",
    "\n",
    "# load the data\n",
    "x,y = load_data(0) # you can load_data(0), load_data(1), or load_data(2)\n",
    "\n",
    "# TODO: examine this data, e.g., plot it, print its shape, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic view of classification\n",
    "\n",
    "Instead of training a model to output a *discrete class* (such as 0 or 1) we can train a model to output a *continuous probability* that some input feature belongs to a certain class. In other words, we want $p(y\\mid x)$. In the case above we want $p(y=1 \\mid x), p(y=0 \\mid x)$, and since these two values add up to one we can train a single model $f(x)$ to predict $p(y=1 \\mid x)$.\n",
    "\n",
    "### Model\n",
    "\n",
    "Our goal is to approximate the 0-1 function with a continuous one. Such a function is called a *sigmoid*, a function which squashes its input to be between 0 and 1. A classic example of such a function is the logistic function, $\\sigma(x) = \\frac{1}{1+e^{-x}}$. And so our model will simply be a linear function of our features followed by a sigmoid: $f(x) = \\sigma(mx + b) = \\frac{1}{1+e^{-mx - b}}$.\n",
    "\n",
    "As previously we will vectorize and thus write, $f(X) = \\sigma(XA) = \\frac{1}{1+e^{-XA}}$.\n",
    "\n",
    "Our model now outputs a value between $0$ and $1$, for a new data point if this value is greater than half we will label it positive ($1$) and otherwise negative ($0$). \n",
    "\n",
    "Implement the `logistic_prediction` function, which should first compute the linear prediction $XA$ then pass that prediction through a sigmoid.\n",
    "\n",
    "Now implement the `logistic_error` function which computes how many predictions are correct by checking whether the probabilities are greater than half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the logistic_error and logistic_prediction functions\n",
    "from mp9 import logistic_error, logistic_prediction\n",
    "from main import plot_logistic_regression_decision_boundary, plot_classification_data\n",
    "\n",
    "A_random = np.random.randn(3, 1) # 3 because we have 2 features and 1 bias term added by get_simple_linear_features\n",
    "y_preds = logistic_prediction(x, A_random, get_simple_linear_features)\n",
    "error = logistic_error(y_preds, y)\n",
    "\n",
    "print(f\"Random model error: {error}\")\n",
    "\n",
    "# plot\n",
    "plot_classification_data(x, y)\n",
    "plot_logistic_regression_decision_boundary(A_random, x, get_simple_linear_features)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_logistic_prediction()\n",
    "tests.test_logistic_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Loss\n",
    "\n",
    "In this probabilistic view our objective changes - instead of minimizing squared error we will maximize the likelihood of seeing the data under our model. In other words, we think of the data as coming from an underlying distribution, and if our learned model were the true one, what is the probability that sampling from that distribution would produce the training data? A good model is one which maximizes this probability. This approach is called **maximum likelihood estimation (MLE)**.\n",
    "\n",
    "Let's start by writing down this probability given our dataset $\\mathcal{D} = \\{(x_i, y_i)\\}_0^n$:\n",
    "$$p(\\mathcal{D}) = p((x_0,y_0) \\cap (x_1,y_1) \\cap \\dots \\cap (x_n, y_n)) = \\prod_i p(y_i \\mid x_i)$$\n",
    "Notice that the second equality there makes an assumption about our data - that each element of the dataset was sampled *independently*. \n",
    "\n",
    "Our goal is to produce an estimator $f$ which maximizes this quantity:\n",
    "$$f = \\argmax_f \\mathcal{L}(f ; \\mathcal{D}) = \\argmax_f \\prod_{i : y_i = 1} f(x_i) \\prod_{i : y_i = -1} 1-f(x_i)$$\n",
    "Notice that learning a single $f$ for the data makes a second assumption - that each element of the dataset was sampled *identically*. \n",
    "\n",
    "Together, the two assumptions above are the core assumptions of MLE, that we have \"independent and identically distributed\" (IID) data. To find the maximizer of $\\mathcal{L}$ we can, as usual, take the derivative. Before we do so we'll make one last observation - in practice products are bad because a large product of terms between $0$ and $1$ causes issues like precision underflow. Imagine our starting model assigns every data point probability $0.5$, this product is then equal to $0.5^n$, which for large datasets will be a very small number, often smaller than the precision available on your machine, and therefore rounded to $0$. Consequently the derivative will also be $0$. Therefore instead of maximizing the product we will use one of the most common tricks in machine learning - take the log. The logarithm is a concave function (monotonically increasing) which means that maximizing $f$ is equivalent to maximizing $\\log(f)$. Moreover, the log of a product is the sum of the logs:\n",
    "$$f = \\argmax_f \\mathcal{L}(f ; \\mathcal{D}) = \\argmax_f \\prod_{i : y_i = 1} f(x_i) \\prod_{i : y_i = -1} 1-f(x_i) = \\\\\n",
    "\\argmax_f \\sum_{i : y_i = 1} \\log(f(x_i)) + \\sum_{i : y_i = -1} \\log(1-f(x_i)) $$\n",
    "\n",
    "One last trick, for convenience, is to combine those two summations into one that uses the labels as part of the loss, and to turn the maximization into a minimization for gradient descent by negating:\n",
    "$$f = \\argmax_f \\mathcal{L}(f ; \\mathcal{D}) = \\argmin_f -\\mathcal{L}(f ; \\mathcal{D}) = \\argmin_f \\sum_i -y_i \\log(f(x_i)) - (1-y_i)\\log(1-f(x_i))$$\n",
    "\n",
    "This loss is called the *negative log likelihood (nll)*. Implement this now inside of `logistic_loss`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the logistic_loss function\n",
    "from mp9 import logistic_loss\n",
    "\n",
    "# notice that our loss is not the same as our error \n",
    "# - error measures how many predictions our model got wrong\n",
    "# - loss measures the (negative log) likelihood of the data given our model\n",
    "# our model (above) is random so naturally the likelihood of the data given our model is low\n",
    "loss = logistic_loss(y_preds, y)\n",
    "print(f\"Random model loss: {loss}\")\n",
    "print(f\"Random model error: {error}\")\n",
    "print(f\"Likelihood of data given random model: {np.exp(-loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_logistic_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Now that we've written down our objective as a loss we can minimize it with gradient descent. We'll use the fact that $\\frac{d}{dx} \\log(x) = \\frac{1}{x}$ and that $\\frac{d}{dx} \\sigma(x) = \\sigma(x)(1-\\sigma(x))$. Finally, recall that $f(X) = \\sigma(XA)$ for parameters $A$.\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial A} = \\sum_i -\\frac{\\partial}{\\partial A} y_i \\log(f(x_i)) - \\frac{\\partial}{\\partial A}(1-y_i)\\log(1-f(x_i)) \\\\ \n",
    "= \\sum_i -\\frac{ y_i}{f(x_i)} \\frac{\\partial}{\\partial A}f(x_i) - \\frac{1-y_i}{1-f(x_i)}\\frac{\\partial}{\\partial A}(1-f(x_i)) \\\\\n",
    "= \\sum_i \\left(\\frac{1-y_i}{1-f(x_i)}-\\frac{y_i}{f(x_i)}\\right) \\frac{\\partial}{\\partial A}f(x_i)\\\\\n",
    "= \\sum_i \\left(\\frac{1-y_i}{1-f(x_i)}-\\frac{y_i}{f(x_i)}\\right) \\sigma(x_iA)(1-\\sigma(x_iA))\\left(\\frac{\\partial}{\\partial A}x_iA\\right)\\\\\n",
    "= \\sum_i \\left(\\frac{1-y_i}{1-\\sigma(x_iA)}-\\frac{y_i}{\\sigma(x_iA)}\\right) \\sigma(x_iA)(1-\\sigma(x_iA))x_i \\\\\n",
    "= \\sum_i ((1-y_i)\\sigma(x_iA) - y_i(1-\\sigma(x_iA)))x_i \\\\\n",
    "= \\sum_i (\\sigma(x_iA) - y_i)x_i$$\n",
    "\n",
    "With vectorization we can simply write this as:\n",
    "$$X^T(\\sigma(XA) - Y) = X^T(Y_{pred} - Y)$$\n",
    "where $Y_{pred}$ is our prediction $f(X) = \\sigma(XA)$.\n",
    "\n",
    "Compute this gradient in `get_logistic_regression_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the get_logistic_regression_gradient function\n",
    "from mp9 import get_logistic_regression_gradient\n",
    "\n",
    "def do_gd_for_logistic(x,y,get_features):\n",
    "    X = get_features(x)\n",
    "    A_init = np.random.randn(X.shape[1], 1)\n",
    "    learning_rate = 0.001\n",
    "    num_iterations = 100000\n",
    "    get_gradient = lambda A: get_logistic_regression_gradient(A, X, y)\n",
    "    predicted_A = gradient_descent(get_gradient, A_init, learning_rate, num_iterations)\n",
    "\n",
    "    # compute final loss and error\n",
    "    y_preds = logistic_prediction(x, predicted_A, get_features)\n",
    "    error = logistic_error(y_preds, y)\n",
    "    loss = logistic_loss(y_preds, y)\n",
    "    print(f\"GD model error: {error}\")\n",
    "    print(f\"GD model loss: {loss}\")\n",
    "    print(f\"Likelihood of data given GD model: {np.exp(-loss)}\")\n",
    "\n",
    "    # plot\n",
    "    plot_classification_data(x, y)\n",
    "    plot_logistic_regression_decision_boundary(predicted_A, x, get_features)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Now do gradient descent\n",
    "x,y = load_data(0)\n",
    "do_gd_for_logistic(x,y,get_simple_linear_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not move on until these tests pass (they should print nothing if they pass)\n",
    "tests.test_logistic_regression_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the second set of data... can you correctly classify it with a linear model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = load_data(1)\n",
    "do_gd_for_logistic(x,y,get_simple_linear_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a linear classification boundary cannot separate this data properly. Let's try our polynomial features from before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do gradient descent\n",
    "x,y = load_data(1)\n",
    "get_features = lambda x: get_polynomial_features(x, degree=2)\n",
    "do_gd_for_logistic(x,y,get_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third classification dataset - better features\n",
    "\n",
    "The features we've used thus far (linear and polynomial) do not work for the third dataset (`load_data(2)`). Your job is now to design a new set of features to classify this third dataset well by filling in the `get_logistic_regression_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp9 import get_logistic_regression_features\n",
    "\n",
    "x, y = load_data(2)\n",
    "do_gd_for_logistic(x, y, get_logistic_regression_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final test - will raise an error if it fails\n",
    "tests.test_get_logistic_regression_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you should now be able to submit mp9.py on Gradescope and get 100%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
